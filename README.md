
# Recursive scraping of legislation + LongBERT fine tuning

This project aims to take in a list of links with a subset of human-labelled rows and output a finetuned longbert model (base allenai/longformer-base-4096) to predict the human labels.



@article{Beltagy2020Longformer,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={arXiv:2004.05150},
  year={2020},
}




